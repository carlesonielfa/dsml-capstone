{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/pmf8l0nn4rs2zw5xdj5370vr0000gn/T/ipykernel_74832/3070975479.py:18: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_temp = pd.read_csv(file_path)\n",
      "/var/folders/sv/pmf8l0nn4rs2zw5xdj5370vr0000gn/T/ipykernel_74832/3070975479.py:18: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_temp = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Definir la subcarpeta\n",
    "#subfolder = '../../data_csv_bicing/'\n",
    "subfolder = 'data/'\n",
    "\n",
    "# Obtener la lista de archivos CSV en la subcarpeta\n",
    "files = os.listdir(subfolder)\n",
    "files = [f for f in files if f.endswith('ESTACIONS.csv')]\n",
    "\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada archivo CSV\n",
    "df_list = []\n",
    "\n",
    "# Leer cada archivo CSV y a√±adirlo a la lista\n",
    "for file in files:\n",
    "    file_path = os.path.join(subfolder, file)\n",
    "    df_temp = pd.read_csv(file_path)\n",
    "    df_list.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "big_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Guardar en formato parquet\n",
    "big_df.to_parquet('data/1_all_data_raw.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer el archivo .parquet \n",
    "df = pd.read_parquet('data/1_all_data_raw.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiar los datos\n",
    " \n",
    "# Eliminar filas donde 'station_id' es nulo y pasarlos a int\n",
    "df = df.dropna(subset=['station_id'])\n",
    "df['station_id'] = df['station_id'].astype(int)\n",
    "\n",
    "df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s')\n",
    "\n",
    "# Timestamp to hour, day, month, year\n",
    "df['hour'] = df['last_reported'].dt.hour\n",
    "df['day'] = df['last_reported'].dt.day\n",
    "df['month'] = df['last_reported'].dt.month\n",
    "df['year'] = df['last_reported'].dt.year\n",
    "\n",
    "# seleccionar columnas relevantes\n",
    "cols_to_keep = ['station_id', 'year', 'month', 'day', 'hour', 'num_bikes_available', 'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike','num_docks_available', 'last_reported']\n",
    "df = df[cols_to_keep]\n",
    "\n",
    "# Merge by taking the mean of the values\n",
    "df = df.groupby(['station_id', 'hour', 'day', 'month', 'year']).mean().reset_index()\n",
    "\n",
    "# Guardar en formato parquet\n",
    "df.to_parquet(\"data/2_all_data_mean_hour.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer el archivo .parquet \n",
    "df_clean = pd.read_parquet('data/2_all_data_mean_hour.parquet')\n",
    "\n",
    "# Cargar datos de las estaciones\n",
    "df_station_info = pd.read_csv('data/Informacio_Estacions_Bicing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_id                              False\n",
      "hour                                    False\n",
      "day                                     False\n",
      "month                                   False\n",
      "year                                    False\n",
      "num_bikes_available                     False\n",
      "num_bikes_available_types.mechanical    False\n",
      "num_bikes_available_types.ebike         False\n",
      "num_docks_available                     False\n",
      "last_reported                           False\n",
      "dtype: bool station_id                False\n",
      "name                      False\n",
      "physical_configuration    False\n",
      "lat                       False\n",
      "lon                       False\n",
      "altitude                  False\n",
      "address                   False\n",
      "post_code                 False\n",
      "capacity                  False\n",
      "is_charging_station       False\n",
      "nearby_distance           False\n",
      "_ride_code_support        False\n",
      "rental_uris                True\n",
      "cross_street               True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay alguna columna relevaante con valores nulos\n",
    "hay_nulos1 = df_clean.isnull().any()\n",
    "hay_nulos2 = df_station_info.isnull().any()\n",
    "\n",
    "print(hay_nulos1, hay_nulos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# se tienen que eliminar filas con numeros de station_id inexistentes en el registro de estaciones.\n",
    "df_merge = df_clean.merge(df_station_info[[\"station_id\", \"capacity\"]],\n",
    "                             on=\"station_id\",\n",
    "                             how=\"inner\")\n",
    "\n",
    "df_merge[\"percentage_docks_available\"] = df_merge[\"num_docks_available\"] / df_merge[\"capacity\"]\n",
    "df_merge.to_parquet(\"data/3_all_data_ctx.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el archivo .parquet \n",
    "df_merge = pd.read_parquet('data/3_all_data_ctx.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO revisar estrategia.\n",
    "\n",
    "prediction_data = pd.DataFrame()\n",
    "\n",
    "for s in df_merge.station_id.unique():\n",
    "    ctx = df_merge.loc[df_merge[\"station_id\"] == s, :]\n",
    "    ctx = ctx.sort_values(by=[\"year\", \"month\", \"day\", \"hour\"],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    for lag in range(1, 5):\n",
    "        ctx.loc[:, f\"ctx-{lag}\"] = ctx.loc[:, \"percentage_docks_available\"].shift(lag)\n",
    "\n",
    "    ctx = ctx.iloc[4::5]\n",
    "\n",
    "    prediction_data = pd.concat([prediction_data, ctx], ignore_index=True)\n",
    "\n",
    "prediction_data = prediction_data[[\"station_id\", \"year\", \"month\", \"day\", \"hour\", \"percentage_docks_available\"]]\n",
    "\n",
    "prediction_data.to_parquet(\"data/4_data_ctx4h_v1.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
