{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "858baa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def remove_datetime_duplicates(df):\n",
    "    l = []\n",
    "    for _, group in df.groupby(\"station_id\"):\n",
    "        station = group.drop_duplicates(subset=\"datetime\", keep=\"first\")\n",
    "        l.append(station)\n",
    "\n",
    "    result = pd.concat(l, ignore_index=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_cleaned_data_parquet(filename_path: str) -> pd.DataFrame:\n",
    "    table = pq.read_table(filename_path)\n",
    "    cleaned_data = table.to_pandas()\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def get_prediction_raw_data(clean_data_file_path,\n",
    "                            station_information_file_path,\n",
    "                            metadata_sample_submission_path,\n",
    "                            bank_holidays_bcn_path,\n",
    "                            meteo_data_path):\n",
    "    cleaned_data = read_cleaned_data_parquet(clean_data_file_path)\n",
    "\n",
    "    station_information = pd.read_csv(station_information_file_path)\n",
    "    metadata_sample_submission = pd.read_csv(metadata_sample_submission_path)\n",
    "    bank_holidays_bcn = pd.read_csv(bank_holidays_bcn_path)\n",
    "    meteo_data = pd.read_csv(meteo_data_path)\n",
    "\n",
    "    # Add capacity for each station_id\n",
    "    merge = cleaned_data.merge(station_information[[\"station_id\", \"capacity\", \"lat\", \"lon\", \"altitude\", \"post_code\"]],\n",
    "                               on=\"station_id\",\n",
    "                               how=\"inner\")\n",
    "\n",
    "    # Filter out station_id not in metadata_sample_submission\n",
    "    metadata_station_id_list = metadata_sample_submission[\"station_id\"].unique()\n",
    "    merge = merge[merge[\"station_id\"].isin(metadata_station_id_list)]\n",
    "\n",
    "    # Ensure that \"num_docks_available\" is not > \"capacity\"\n",
    "    merge.loc[merge[\"num_docks_available\"] > merge[\"capacity\"], \"num_docks_available\"] = merge.loc[\n",
    "        merge[\"num_docks_available\"] > merge[\"capacity\"], \"capacity\"]\n",
    "\n",
    "    # Create percentage docks available\n",
    "    merge[\"percentage_docks_available\"] = merge[\"num_docks_available\"] / merge[\"capacity\"]\n",
    "\n",
    "    # Filter out records before 2020\n",
    "    merge = merge[merge[\"year\"] >= 2020]\n",
    "\n",
    "    # Create datetime column\n",
    "    merge['datetime'] = pd.to_datetime(merge[['year', 'month', 'day', 'hour']].astype(str).agg('-'.join, axis=1),\n",
    "                                       format='%Y-%m-%d-%H')\n",
    "\n",
    "    merge = merge[[\"station_id\", \"lat\", \"lon\", \"altitude\", \"post_code\", \"year\", \"month\", \"day\",\n",
    "                   \"hour\", \"num_docks_available\", \"capacity\",\n",
    "                   \"percentage_docks_available\", \"datetime\"]]\n",
    "\n",
    "    # Remove duplicates\n",
    "    merge = remove_datetime_duplicates(merge)\n",
    "\n",
    "    # Ensure that all the station_id have all dates from min_data to max_data\n",
    "    list_df_station = []\n",
    "    for s in merge.station_id.unique():\n",
    "        df_station = merge[merge[\"station_id\"] == s]\n",
    "        lat = df_station[\"lat\"].iloc[0]\n",
    "        lon = df_station[\"lon\"].iloc[0]\n",
    "        altitude = df_station[\"altitude\"].iloc[0]\n",
    "        post_code = df_station[\"post_code\"].iloc[0]\n",
    "        capacity = df_station[\"capacity\"].iloc[0]\n",
    "        datetime_available = df_station[\"datetime\"].unique()\n",
    "        min_data = min(datetime_available)\n",
    "        max_data = max(datetime_available)\n",
    "\n",
    "        complete_datetime_range = pd.date_range(start=min_data, end=max_data, freq='h')\n",
    "        missing_datetime = list(set(complete_datetime_range) - set(datetime_available))\n",
    "\n",
    "        rows_to_add = []\n",
    "        for d in missing_datetime:\n",
    "            rows_to_add.append({\n",
    "                \"station_id\": s,\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon,\n",
    "                \"altitude\": altitude,\n",
    "                \"post_code\": post_code,\n",
    "                \"year\": d.year,\n",
    "                \"month\": d.month,\n",
    "                \"day\": d.day,\n",
    "                \"hour\": d.hour,\n",
    "                \"num_docks_available\": np.nan,\n",
    "                \"capacity\": capacity,\n",
    "                \"datetime\": d})\n",
    "\n",
    "        new_rows = pd.DataFrame(rows_to_add)\n",
    "\n",
    "        df_station = pd.concat([df_station, new_rows], ignore_index=True)\n",
    "\n",
    "        list_df_station.append(df_station)\n",
    "\n",
    "    prediction_raw_data = pd.concat(list_df_station, ignore_index=True)\n",
    "\n",
    "    # Add bank holidays\n",
    "    prediction_raw_data[\"date\"] = prediction_raw_data[\"datetime\"].dt.date\n",
    "    bank_holidays_bcn[\"holiday_date\"] = pd.to_datetime(bank_holidays_bcn[\"holiday_date\"]).dt.date\n",
    "\n",
    "    holiday_dates_list = bank_holidays_bcn[\"holiday_date\"].unique()\n",
    "    prediction_raw_data[\"is_holidays\"] = prediction_raw_data[\"date\"].isin(holiday_dates_list)\n",
    "\n",
    "    # Add meteo data\n",
    "    meteo_data['data'] = pd.to_datetime(meteo_data['data']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    meteo_data['data'] = pd.to_datetime(meteo_data['data'])\n",
    "\n",
    "    prediction_raw_data = prediction_raw_data.merge(meteo_data[[\"data\", \"calor\", \"lluvia\"]], left_on=\"datetime\",\n",
    "                                                    right_on=\"data\", how=\"left\").drop(columns=\"data\")\n",
    "\n",
    "    # Transform dtype columns\n",
    "    prediction_raw_data[\"calor\"] = prediction_raw_data[\"calor\"].fillna(0).astype(bool)\n",
    "    prediction_raw_data[\"lluvia\"] = prediction_raw_data[\"lluvia\"].fillna(0).astype(bool)\n",
    "\n",
    "    prediction_raw_data[\"station_id\"] = prediction_raw_data[\"station_id\"].astype(str)\n",
    "    prediction_raw_data[\"post_code\"] = prediction_raw_data[\"post_code\"].astype(str)\n",
    "\n",
    "    #  Export dataFrame\n",
    "    prediction_raw_data.to_parquet(\"../data/prediction_raw_data.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0e70ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 4194304 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m read_cleaned_data_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cleaned_data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m station_information \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/station_information.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m metadata_sample_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/metadata_sample_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mread_cleaned_data_parquet\u001b[1;34m(filename_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_cleaned_data_parquet\u001b[39m(filename_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m---> 18\u001b[0m     table \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mread_table(filename_path)\n\u001b[0;32m     19\u001b[0m     cleaned_data \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2973\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2962\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2963\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   2964\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2965\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2970\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   2971\u001b[0m         )\n\u001b[1;32m-> 2973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mread(columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39muse_threads,\n\u001b[0;32m   2974\u001b[0m                         use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[0;32m   2976\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2977\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2979\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   2982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2601\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2593\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2594\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2595\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2596\u001b[0m         ]\n\u001b[0;32m   2597\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2598\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   2599\u001b[0m         )\n\u001b[1;32m-> 2601\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   2602\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_expression,\n\u001b[0;32m   2603\u001b[0m     use_threads\u001b[38;5;241m=\u001b[39muse_threads\n\u001b[0;32m   2604\u001b[0m )\n\u001b[0;32m   2606\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:369\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:2818\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 4194304 failed"
     ]
    }
   ],
   "source": [
    "cleaned_data = read_cleaned_data_parquet(\"../data/cleaned_data.parquet\")\n",
    "\n",
    "station_information = pd.read_csv(\"../data/station_information.csv\")\n",
    "metadata_sample_submission = pd.read_csv(\"../data/metadata_sample_submission.csv\")\n",
    "bank_holidays_bcn = pd.read_csv(\"../data/bank_holidays_bcn.csv\")\n",
    "meteo_data = pd.read_csv(\"../data/valores_booleanos_meteo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743b09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add capacity for each station_id\n",
    "merge = cleaned_data.merge(station_information[[\"station_id\", \"capacity\", \"lat\", \"lon\", \"altitude\", \"post_code\"]],\n",
    "                               on=\"station_id\",\n",
    "                               how=\"inner\")\n",
    "\n",
    "# Filter out station_id not in metadata_sample_submission\n",
    "metadata_station_id_list = metadata_sample_submission[\"station_id\"].unique()\n",
    "merge = merge[merge[\"station_id\"].isin(metadata_station_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1cb5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that \"num_docks_available\" is not > \"capacity\"\n",
    "merge.loc[merge[\"num_docks_available\"] > merge[\"capacity\"], \"num_docks_available\"] = merge.loc[merge[\"num_docks_available\"] > merge[\"capacity\"], \"capacity\"]\n",
    "\n",
    "# Create percentage docks available\n",
    "merge[\"percentage_docks_available\"] = merge[\"num_docks_available\"] / merge[\"capacity\"]\n",
    "\n",
    "# Filter out records before 2020\n",
    "merge = merge[merge[\"year\"] >= 2020]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95cb422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime column\n",
    "merge['datetime'] = pd.to_datetime(merge[['year', 'month', 'day', 'hour']].astype(str).agg('-'.join, axis=1),format='%Y-%m-%d-%H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38faf959",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = merge[[\"station_id\", \"lat\", \"lon\", \"altitude\", \"post_code\", \"year\", \"month\", \"day\",\n",
    "                   \"hour\", \"num_docks_available\", \"capacity\",\n",
    "                   \"percentage_docks_available\", \"datetime\"]]\n",
    "\n",
    "# Remove duplicates\n",
    "merge = remove_datetime_duplicates(merge)\n",
    "\n",
    "# Ensure that all the station_id have all dates from min_data to max_data\n",
    "list_df_station = []\n",
    "for s in merge.station_id.unique():\n",
    "    df_station = merge[merge[\"station_id\"] == s]\n",
    "    lat = df_station[\"lat\"].iloc[0]\n",
    "    lon = df_station[\"lon\"].iloc[0]\n",
    "    altitude = df_station[\"altitude\"].iloc[0]\n",
    "    post_code = df_station[\"post_code\"].iloc[0]\n",
    "    capacity = df_station[\"capacity\"].iloc[0]\n",
    "    datetime_available = df_station[\"datetime\"].unique()\n",
    "    min_data = min(datetime_available)\n",
    "    max_data = max(datetime_available)\n",
    "\n",
    "    complete_datetime_range = pd.date_range(start=min_data, end=max_data, freq='h')\n",
    "    missing_datetime = list(set(complete_datetime_range) - set(datetime_available))\n",
    "\n",
    "    rows_to_add = []\n",
    "    for d in missing_datetime:\n",
    "        rows_to_add.append({\n",
    "            \"station_id\": s,\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "            \"altitude\": altitude,\n",
    "            \"post_code\": post_code,\n",
    "            \"year\": d.year,\n",
    "            \"month\": d.month,\n",
    "            \"day\": d.day,\n",
    "            \"hour\": d.hour,\n",
    "            \"num_docks_available\": np.nan,\n",
    "            \"capacity\": capacity,\n",
    "            \"datetime\": d})\n",
    "\n",
    "    new_rows = pd.DataFrame(rows_to_add)\n",
    "\n",
    "    df_station = pd.concat([df_station, new_rows], ignore_index=True)\n",
    "\n",
    "    list_df_station.append(df_station)\n",
    "\n",
    "prediction_raw_data = pd.concat(list_df_station, ignore_index=True)\n",
    "\n",
    "# Add bank holidays\n",
    "prediction_raw_data[\"date\"] = prediction_raw_data[\"datetime\"].dt.date\n",
    "bank_holidays_bcn[\"holiday_date\"] = pd.to_datetime(bank_holidays_bcn[\"holiday_date\"]).dt.date\n",
    "\n",
    "holiday_dates_list = bank_holidays_bcn[\"holiday_date\"].unique()\n",
    "prediction_raw_data[\"is_holidays\"] = prediction_raw_data[\"date\"].isin(holiday_dates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a559594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add meteo data\n",
    "meteo_data['data'] = pd.to_datetime(meteo_data['data']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "meteo_data['data'] = pd.to_datetime(meteo_data['data'])\n",
    "\n",
    "prediction_raw_data = prediction_raw_data.merge(meteo_data[[\"data\", \"calor\", \"lluvia\",\"dia\"]], left_on=\"datetime\",\n",
    "                                                    right_on=\"data\", how=\"left\").drop(columns=\"data\")\n",
    "\n",
    "# Transform dtype columns\n",
    "prediction_raw_data[\"calor\"] = prediction_raw_data[\"calor\"].fillna(0).astype(bool)\n",
    "prediction_raw_data[\"lluvia\"] = prediction_raw_data[\"lluvia\"].fillna(0).astype(bool)\n",
    "\n",
    "prediction_raw_data[\"station_id\"] = prediction_raw_data[\"station_id\"].astype(str)\n",
    "prediction_raw_data[\"post_code\"] = prediction_raw_data[\"post_code\"].astype(str)\n",
    "\n",
    "#  Export dataFrame\n",
    "prediction_raw_data.to_parquet(\"../data/prediction_raw_data.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
